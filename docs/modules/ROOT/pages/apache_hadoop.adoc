= Apache Hadoop

Apache Hadoop is a framework that allows distributed processing of large data sets across clusters of computers using simple programming models.

The core of Hadoop consist of

- Hadoop Distributed File System: A distributed file system that runs on standard or low-end hardware.
- MapReduce: A framework that helps programs do parallel computation on data.
- Yet Another Resource Negotiator: Manages and monitors cluster nodes and resource usage; Schedule jobs and tasks

Hadoop provides building blocks for other services and applications, making it easier to use all the storage and processing capacity in cluster servers and to execute distributed processes against huge amounts of data.

Hadoop cluster are composed of a network of Hadoop Master Nodes and Hadoop Worker Node.

The Hadoop Master Nodes typically utilize higher-quality hardware and include a Hadoop NameNode, Hadoop Secondary NameNode, and Hadoop JobTracker.

The Hadoop Worker Nodes typically run both Hadoop DataNode and Hadoop TaskTracker services on commodity hardware, and do actual work of storing and processing the jobs as directed by the master nodes.

The term Hadoop also used to refers to the ecosystem, which includes additional software packages such as Apache Pig, xref:apache_hive.adoc[], Apache HBase, xref:apache_spark.adoc[], xref:presto.adoc[], etc.
